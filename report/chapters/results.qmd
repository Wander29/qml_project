# Results

I ran multiple test with various ansatzes. For ansatzes naming keep in mind @fig-conv_ansatzes and @fig-pool-ansatz.

## Convolution ansatz (a)
For convolution ansatz (a) and pooling ansatz (1) I obtained the following results:

```{python}
#| echo: false
#| tbl-cap: a
#| tbl-colwidths: [40, 30, 30]
#| output: asis

import pandas as pd
df = pd.read_csv("../../scripts/results/out_2024-03-27_12-03-11.csv")
df.sort_values(by=['mean_test_score'], ascending=False, inplace=True)

a = pd.concat( 
    (
      df["param_model__share_weights"], 
      df["param_model__stride_c"], 
      df["param_model__step_c"], 
      df["param_model__filter_p"], 
      df["param_model__stride_p"], 
      df["mean_test_score"]) ,
  axis=1)
a.columns = ["Share weights", "Conv. stride", "Conv. step", "Pool. filter", "Pool. stride", "Mean validation score"]
a

```

The best configuration with `share_weights=True` selected was:

- `convolution_stride`: 7
- `convolution_step`: 1
- `pooling filter`: !*!
- `pooling stride`: 3

While the best configuration with `share_weights=False` selected was:

- `convolution_stride`: 1
- `convolution_step`: 1
- `pooling filter`: *!
- `pooling stride`: 2

Measuring them with the test set I obtained:

```{python}
#| echo: False
df = pd.DataFrame( {
  'share weights': [False, True],
  "accuracy":[0.83, 0.7],
  "parameters": [32,12]
})
df
```

These are the results with the same convolution ansatz but with ansatz pooling (2):


```{python}
#| echo: False
import pandas as pd
df = pd.read_csv("../../scripts/results/out_2024-03-28_07-40-00.csv")
df.sort_values(by=['mean_test_score'], ascending=False, inplace=True)

a = pd.concat( 
    (
      df["param_model__share_weights"], 
      df["param_model__stride_c"], 
      df["param_model__step_c"], 
      df["param_model__filter_p"], 
      df["param_model__stride_p"], 
      df["mean_test_score"]) ,
  axis=1)
a.columns = ["Share weights", "Conv. stride", "Conv. step", "Pool. filter", "Pool. stride", "Mean validation score"]
a.head(6)
```

Since there are multiple configurations with the same validation score I try two of them on the test set:
```{python}
#| echo: False
df = pd.DataFrame( {
  'configuration': ["1st", "4th"],
  "accuracy":[0.78, 0.8],
  "parameters": [26, 14]
})
df
```

Configuration number is referred to results above.

## Convolution ansatz (b)
These are the results with pooling ansatz (1).
```{python}
#| echo: False
import pandas as pd
df = pd.read_csv("../../scripts/results/out_2024-03-28_08-28-01.csv")
df.sort_values(by=['mean_test_score'], ascending=False, inplace=True)

a = pd.concat( 
    (
      df["param_model__stride_c"], 
      df["param_model__step_c"], 
      df["param_model__filter_p"], 
      df["param_model__stride_p"], 
      df["mean_test_score"]) ,
  axis=1)
a.columns = ["Conv. stride", "Conv. step", "Pool. filter", "Pool. stride", "Mean validation score"]
a.head(6)
```

```{python}
#| echo: False
df = pd.DataFrame( {
  'configuration': ["1st", "2nd"],
  "accuracy":[0.683, 0.75],
  "parameters": [32, 20]
})
df
```


Below there are the results with pooling ansatz (2). With `share_weights=False`:
```{python}
#| echo: False
import pandas as pd
df = pd.read_csv("../../scripts/results/out_2024-03-29_03-20-47.csv")
df = df.loc[ 
  (df['param_model__ansatz_c'] == 'b') & 
  (df['param_model__ansatz_p'] == 2) & 
  (df['param_model__share_weights'] == False)
]
df.sort_values(by=['mean_test_score'], ascending=False, inplace=True)

a = pd.concat( 
    (
      df["param_model__stride_c"], 
      df["param_model__step_c"], 
      df["param_model__filter_p"], 
      df["param_model__stride_p"], 
      df["mean_test_score"]) ,
  axis=1)
a.columns = ["Conv. stride", "Conv. step", "Pool. filter", "Pool. stride", "Mean validation score"]
a.head(10)
```

```{python}
#| echo: False
df = pd.DataFrame( {
  'configuration': ["1st"],
  "accuracy":[0.7],
  "parameters": [26]
})
df
```

Yet with pooling ansatz (2) but setting `share_weights=True`:
```{python}
#| echo: False
import pandas as pd
df = pd.read_csv("../../scripts/results/out_2024-03-29_03-20-47.csv")
df = df.loc[ 
  (df['param_model__ansatz_c'] == 'b') & 
  (df['param_model__ansatz_p'] == 2) & 
  (df['param_model__share_weights'] == True)
]
df.sort_values(by=['mean_test_score'], ascending=False, inplace=True)

a = pd.concat( 
    (
      df["param_model__stride_c"], 
      df["param_model__step_c"], 
      df["param_model__filter_p"], 
      df["param_model__stride_p"], 
      df["mean_test_score"]) ,
  axis=1)
a.columns = ["Conv. stride", "Conv. step", "Pool. filter", "Pool. stride", "Mean validation score"]
a.head(7)
```

```{python}
#| echo: False
df = pd.DataFrame( {
  'configuration': ["1st", "2nd", "3rd", "4th", "5th"],
  "accuracy":[0.717, 0.733, 0.683, 0.6, 0.633],
  "parameters": [6, 6, 6, 6, 6]
})
df
```

The second configuration appears to be the best minimal architecture.

## Convolution ansatz (g)
For convolution ansatz (g) I decided to fix `share_weights=True` since the number of parameters would have grown from 30 to 130.

These are the results with pooling ansatz (1).
```{python}
#| echo: False
import pandas as pd
df = pd.read_csv("../../scripts/results/out_2024-03-28_09-18-22.csv")
df.sort_values(by=['mean_test_score'], ascending=False, inplace=True)

a = pd.concat( 
    (
      df["param_model__stride_c"], 
      df["param_model__step_c"], 
      df["param_model__filter_p"], 
      df["param_model__stride_p"], 
      df["mean_test_score"]) ,
  axis=1)
a.columns = ["Conv. stride", "Conv. step", "Pool. filter", "Pool. stride", "Mean validation score"]
a.head(6)
```

I tried the first 3 configurations on the test set:

```{python}
#| echo: False
df = pd.DataFrame( {
  'configuration': ["1st", "2nd", "3rd"],
  "accuracy":[0.75, 0.867, 0.75],
  "parameters": [36, 36, 36]
})
df
```

After discovering the high accuracy of the 2nd configuration I also tried to set `share_weight=False` only for this particular configuration and this is the result I obtained on the test set:

```{python}
#| echo: False
df = pd.DataFrame( {
  'share weights': [False, True],
  "accuracy":[0.883, 0.867],
  "parameters": [136, 36]
})
df
```

I found these last two to be the most interesting architectures, in particular I'd choose the architecture with `share_weights=True` according to the principle of parsimony.

Below there are also the less interesting results with pooling ansatz (2) (always with `share_weights=True`):
```{python}
#| echo: False
import pandas as pd
df = pd.read_csv("../../scripts/results/out_2024-03-29_03-20-47.csv")
df = df.loc[ 
  (df['param_model__ansatz_c'] == 'g') & 
  (df['param_model__ansatz_p'] == 2) & 
  (df['param_model__share_weights'] == True)
]
df.sort_values(by=['mean_test_score'], ascending=False, inplace=True)

a = pd.concat( 
    (
      df["param_model__stride_c"], 
      df["param_model__step_c"], 
      df["param_model__filter_p"], 
      df["param_model__stride_p"], 
      df["mean_test_score"]) ,
  axis=1)
a.columns = ["Conv. stride", "Conv. step", "Pool. filter", "Pool. stride", "Mean validation score"]
a.head(10)
```

```{python}
#| echo: False
df = pd.DataFrame( {
  'configuration': ["1st", "2nd", "3rd", "4th"],
  "accuracy":[0.733, 0.783, 0.733, 0.7],
  "parameters": [30, 30, 30, 30]
})
df
```