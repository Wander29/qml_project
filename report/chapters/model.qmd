# Model implementation

## Dataset
I use the dataset GTZAN from Kaggle @GTZAN_kaggle.

## QCNN architecture

![Main workflow... @lourens2023hierarchical](./img/workflow.png)

Circuit

```{python}
#| label: fig-code_circuit
#| eval: False
#| code-line-numbers: True

def get_circuit(hierq, x=None):
    dev = qml.device("default.qubit.torch", wires=hierq.tail.Q, shots=None)

    @qml.qnode(dev, interface="torch", diff_method="backprop")
    def circuit():
        if isinstance(next(hierq.get_symbols(), False), sp.Symbol):
            # Pennylane doesn't support symbolic parameters, so if no symbols were set (i.e. they are still symbolic), we initialize them randomly
            hierq.set_symbols(np.random.uniform(0, 2 * np.pi, hierq.n_symbols))
        if x is not None:
            AngleEmbedding(x, wires=hierq.tail.Q, rotation="Y")
        hierq(backend="pennylane")  # This executes the compute graph in order
        return qml.probs(wires=hierq.head.Q[0])

    return circuit

```

The code below illustrates the motif built using HierarQcal.
```{python}
#|eval: false
def qcnn_motif(ansatz_c, conv_stride, conv_step, conv_offset, share_weights, ansatz_p, pool_filter):
    qcnn = (
        Qinit(8)
        + (
            Qcycle(
                stride=conv_stride,
                step=conv_step,
                offset=conv_offset,
                mapping=ansatz_c,
                share_weights=share_weights,
            )
            + Qmask(pool_filter, mapping=ansatz_p)
        )
        * 3
    )

    return qcnn
```


```{python}
#| echo: false
#| tbl-cap: a
#| tbl-colwidths: [40, 30, 30]
#| output: asis

import pandas as pd
df = pd.read_csv("../../results/out_2024-03-25_11-21-56.csv")
a = pd.concat((df["param_model__stride_c"], pd.DataFrame(df["param_model__filter_p"]), df["mean_test_score"]), axis=1)
a.columns = ["Conv. stride", "Pool. filter", "Mean validation score"]
a
```

```{python}
#| column: page
#| echo: false
df = pd.read_csv("../../results/out_2024-03-25_11-21-56.csv")
df
```

The model was trained without batch for 100 epochs employing the Adam optimizer with a learning rate of $1×10−1$ that minimizes the Cross-Entropy Loss. All experiments were performed using Pytorch and the PennyLane Quantum library @pennylane on an AMD Ryzen 7 PRO 5850U with 16GB of RAM.